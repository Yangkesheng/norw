# Kafka

* 数据是按照一定顺序持久化保存。
* 数据分布在整个系统里，具备数据故障保护和性能伸缩能力。
* Kafka 的数据单元被称为消息。消息由字节数组组成，消息里的数据没有特别的格式或含义。
* 为了提高效率，消息被分批次写入 Kafka。批次就是一组消息，这些消息属于同一个主题和分区。
* 主题就好比数据库的表，或者文件系统里的文件夹。主题可以被分为若干个分区，一个分区就是一个提交日志。消息以追加的方式写入分区，然后以先入先出的顺序读取。
* 群组保证每个分区只能被一个消费者使用。

### broker和集群

* 一个独立的 Kafka 服务器被称为 broker。
* borker是集群的组成单位
* 分区复制
* 保留消息：要么保留一段时间（比如7天）、要么保留到消息数量达到上限。主题可以配置自己的保留策略

### 多集群

为什么使用多集群？

1. 数据类型分离
2. 安全需求隔离
3. 多数据中心（灾难恢复）

### 为什么选择kafka

1. 多个生产者
2. 多个消费者
3. 基于磁盘的数据存储
4. 伸缩性
5. 高性能

## 数据生态系统

#### 使用场景

1. 活动跟踪，比如页面访问次数和点击量
2. 传递消息
3. 度量指标和日志记录
4. 提交日志
5. 流处理

# Kafka生产者

## 	创建kafka生产者

#### 发送消息方式

1. 发送并忘记
2. 同步发送
3. 异步发送

## 分区

kafka的消息是一个键值对。

键的作用：

1. 可以作为消息的附加信息
2. 决定消息该被写到主题的那个分区，拥有相同键的消息被写到同一个分区

分区规则：

1. 如果键值为 null，并且使用了默认的分区器，那么记录将被随机地发送到主题内各个可用的分区上。分区器使用轮询（Round Robin）算法将消息均衡地分布到各个分区上。
2. 如果键不为空，并且使用了默认的分区器，那么 Kafka 会对键进行散列（使用 Kafka 自己的散列算法，即使升级 Java 版本，散列值也不会发生变化），然后根据散列值把消息映射到特定的分区上。这里的关键之处在于，同一个键总是被映射到同一个分区上，所以在进行映射时，我们会使用主题所有的分区，而不仅仅是可用的分区。这也意味着，如果写入数据的分区是不可用的，那么就会发生错误。但这种情况很少发生。


# Kafka消费者

### 消费者和消费群组

消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接收主题的一部分分区消息。

### 	消费者群组和分区再均衡

* 当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。在主题发生变化时，比如管理员添加了新的分区，会发生分区重分配。

* 消费者通过向被指派为群组协调器的 broker（不同的群组可以有不同的协调器）发送心跳来维持它们和群组的从属关系以及它们对分区的所有权关系。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发一次再均衡。

#### 分配分区过程



### 创建消费者

### 订阅主题

### 轮询

消息轮询是消费者 API 的核心，通过一个简单的轮询向服务器请求数据。

* 群组协调
* 分区再均衡
* 发送心跳
* 获取数据

### 提交和偏移量

* 自动提交
* 提交当前偏移量
* 异步提交
* 同步和组合异步提交
* 提交特定的偏移量

### 独立的消费者

## 集群成员关系

kafka使用zookeeper来维护集群成员的信息

* 每个broker都有唯一标识

## 控制器

其实就是一个broker，具有一般broker的功能之外，还负责首领的选举。集群里第一个启动的 broker 通过在 Zookeeper 里创建一个临时节点 /controller 让自己成为控制器。

## 复制

* 首领副本：每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。
* 跟随者副本：首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领发生崩溃，其中的一个跟随者会被提升为新首领。

## 处理请求

broker 的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求。

* 元数据请求
* 生产请求
* 获取请求
* 其他请求

## 物理存储

### 文件管理

1. 轮询的分配把首领分区分配到不同broker
2. 同时轮询也把首领的副本分配到下一个broker

### 文件管理

* 大文件查找困难，分区分成若干个片段（默认情况下，每个片段包含 1GB 或一周的数据，以较小的那个为准。）
* 当前写入的片段为活跃片段，活跃片段永远不会被删除。这里边的过期数据不会被删除，直到写满关闭文件

### 文件格式

* kafka消息和偏移量保存在文件里
* 保存在磁盘上的数据 格式从生产者发送过来或者发送给消费者的消息格式是一样的。

### 索引

帮助 broker 更快地定位到指定的偏移量，Kafka 为每个分区维护了一个索引。

### 清理

# 可靠的数据传递

## 可靠性保证

* 保证分区消息的顺序
* 只有当消息被写入分区的所有同步副本时，才被认为是“已提交”
* 只要还有一个副本是活跃的，那么已经提交的消息就不会丢失
* 消息者只能读取已经提交的消息

6.3.2
